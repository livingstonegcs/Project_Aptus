{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "batch_size = 64\n",
    "num_inputs = 784\n",
    "num_outputs = 10\n",
    "def transform(data, label):\n",
    "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
    "train_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=True, transform=transform),\n",
    "                                      batch_size, shuffle=True)\n",
    "test_data = mx.gluon.data.DataLoader(mx.gluon.data.vision.MNIST(train=False, transform=transform),\n",
    "                                     batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_fc = 512\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv2D(channels=20, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    net.add(gluon.nn.Conv2D(channels=50, kernel_size=5, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(num_fc, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for data, _ in train_data:\n",
    "    #print(data, _)\n",
    "    data = data.as_in_context(ctx)\n",
    "    #net(data)\n",
    "    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[-0.04939325 -0.06886163  0.06180146 -0.01860507 -0.02464124  0.10529079\n",
       "  -0.00901893 -0.02111305  0.03408366  0.02087155]\n",
       " [-0.07869698 -0.05847351  0.06795552  0.00685343  0.01523226  0.06278206\n",
       "  -0.05782207 -0.03769309  0.03137237  0.00930196]\n",
       " [-0.08020376 -0.05310691  0.0544848  -0.03762989 -0.01990138  0.08592914\n",
       "  -0.03051051 -0.02974754  0.02314548  0.04224828]\n",
       " [-0.07545905 -0.04583813  0.02926309 -0.02154953 -0.03762564  0.07140541\n",
       "  -0.02922289 -0.0334962   0.02987644  0.03117188]\n",
       " [-0.04873554 -0.01218721 -0.00224156 -0.02325481 -0.02204832  0.06563609\n",
       "  -0.03427698 -0.04230903  0.04004251  0.03027182]\n",
       " [-0.07599139 -0.07418565  0.04866776 -0.00996435  0.00289057  0.06156068\n",
       "  -0.00536753 -0.0439699   0.01910952  0.02695094]\n",
       " [-0.04917657 -0.06767771  0.02160394 -0.01115111  0.02097785  0.08471649\n",
       "  -0.00558962 -0.08215678  0.01659472  0.04256475]\n",
       " [-0.0889467  -0.06539282  0.00112817 -0.01424787 -0.01165471  0.1267069\n",
       "  -0.08280551 -0.04471296  0.08787413  0.04084609]\n",
       " [-0.05135477 -0.01861688 -0.01151436 -0.02445694 -0.00147883  0.05186503\n",
       "  -0.02748413 -0.04258478  0.0330129   0.06797405]\n",
       " [-0.03774799 -0.06068517 -0.00595758  0.03104535  0.01295779  0.06577554\n",
       "  -0.03691056 -0.03882651 -0.00443241  0.01226736]\n",
       " [-0.04550486 -0.06733309  0.01137867  0.01385441  0.00880465  0.086355\n",
       "  -0.011142   -0.04298903  0.00636274  0.04361157]\n",
       " [-0.04884668 -0.08063659  0.03822654  0.00910738  0.00251268  0.06466017\n",
       "  -0.03183284 -0.02037963  0.01896985  0.04118762]\n",
       " [-0.05342878 -0.05763096  0.04252265 -0.0250487  -0.05518231  0.11742476\n",
       "  -0.00640724 -0.02336007  0.03429632 -0.01959409]\n",
       " [-0.07287288 -0.05376223  0.03822519 -0.03113943  0.00945245  0.08441696\n",
       "  -0.05261523 -0.085127    0.00639542  0.0416125 ]\n",
       " [-0.06074702 -0.05783249  0.01027704  0.00379906 -0.00185818  0.06329221\n",
       "  -0.05695696 -0.04020295  0.01594884  0.00837523]\n",
       " [-0.0897647  -0.04362368  0.02241616 -0.02184375  0.00464742  0.04436472\n",
       "  -0.05048145 -0.06287426  0.01381453  0.05463023]\n",
       " [-0.05269881 -0.08017144  0.0286731  -0.0153617  -0.02305551  0.07013918\n",
       "  -0.01555001 -0.0479021   0.04116576  0.04252224]\n",
       " [-0.05110341 -0.02886666  0.06378144 -0.00778748  0.02052104  0.01178363\n",
       "  -0.04207733 -0.08434811  0.02818168  0.03974853]\n",
       " [-0.03911913 -0.09581175  0.03628239 -0.0201599  -0.04206949  0.13909987\n",
       "  -0.03508244 -0.01640565  0.04189527  0.02073365]\n",
       " [-0.0537074  -0.06366131  0.00528603  0.01120188 -0.00039077  0.0624641\n",
       "   0.00711778 -0.01994853  0.01567306  0.03074606]\n",
       " [-0.06366918 -0.01213663  0.00306113 -0.01068396 -0.02947458  0.08622903\n",
       "  -0.0441501  -0.03066878  0.06100845  0.04779126]\n",
       " [-0.07409853 -0.01763074  0.03023472 -0.02303685 -0.03884003  0.06613344\n",
       "  -0.04794192 -0.04576713  0.05498229  0.04276642]\n",
       " [-0.03121029 -0.06005764  0.0234545   0.02918954  0.00631052  0.08095712\n",
       "  -0.0058062  -0.01441227  0.03486393  0.02374891]\n",
       " [-0.07672383 -0.06789526  0.02604554 -0.00978047 -0.04477347  0.10167655\n",
       "  -0.03491258 -0.03130218  0.04561346  0.03953399]\n",
       " [-0.09812145 -0.0443412   0.03697862 -0.00177783  0.05033214  0.02049117\n",
       "  -0.06307851 -0.07294983 -0.00195064  0.0505238 ]\n",
       " [-0.05993075 -0.08257017  0.06897455 -0.04179048 -0.0084013   0.07643633\n",
       "  -0.0516026  -0.09825855  0.02369716  0.01358968]\n",
       " [-0.03573189 -0.10312198  0.039639    0.01467554  0.03607848  0.08275113\n",
       "  -0.02617216 -0.02494314  0.00787502  0.05645054]\n",
       " [-0.0969938  -0.00572926  0.02898252 -0.01587834  0.00912505  0.0456497\n",
       "  -0.07182202 -0.06150823  0.00128145  0.08123942]\n",
       " [-0.05624225 -0.07412595  0.03692786 -0.01710137  0.00176983  0.07094491\n",
       "  -0.01926789 -0.00989069  0.03175732  0.00532132]\n",
       " [-0.08494437 -0.08215842  0.01495022  0.00076473  0.01696554  0.0689378\n",
       "  -0.00829547 -0.06897766 -0.01531244  0.0432419 ]\n",
       " [-0.0647638  -0.08423452  0.03516132 -0.00759961  0.04479672  0.07338874\n",
       "   0.01795296 -0.09974923  0.00643086  0.04843054]\n",
       " [-0.10941681 -0.06804005  0.02141517 -0.04402127 -0.00322612  0.08155651\n",
       "  -0.05781693 -0.07528255  0.00717976  0.06400303]\n",
       " [-0.07188335 -0.02241788  0.01381006 -0.01975691 -0.0439961   0.06776664\n",
       "  -0.03573927 -0.03287658  0.03918984  0.02167   ]\n",
       " [-0.04057685 -0.05682024  0.05827584 -0.01137295 -0.02623323  0.06894669\n",
       "  -0.04052556 -0.05832672  0.01446083  0.02106357]\n",
       " [-0.03682656 -0.04479308  0.02552857 -0.00393965  0.00129791  0.09471838\n",
       "  -0.03158573 -0.03393879  0.01967091  0.03060265]\n",
       " [ 0.00724487 -0.08495741  0.06273147  0.00161456 -0.04379974  0.07127002\n",
       "  -0.02237338 -0.06374206  0.01486759  0.03394109]\n",
       " [-0.04749015 -0.0655347   0.06386349 -0.02950125  0.02909393  0.04148045\n",
       "  -0.00066768 -0.0635324   0.00553455  0.0124489 ]\n",
       " [-0.05234268 -0.0763392   0.07865654 -0.03472488  0.05019898  0.04570623\n",
       "  -0.01726847 -0.0977308  -0.01922437 -0.00198395]\n",
       " [-0.10364069 -0.07920676  0.00090301 -0.01835719 -0.0043955   0.07179078\n",
       "  -0.06624424 -0.07214632  0.02156386  0.05340407]\n",
       " [-0.0260025  -0.04193855  0.01787825  0.01537667 -0.02326632  0.0761406\n",
       "  -0.00793925 -0.0044759   0.02684552  0.01749345]\n",
       " [-0.06634968 -0.05924837  0.03157273 -0.01201302 -0.03060442  0.12047889\n",
       "  -0.04507203 -0.04251155  0.06920094  0.03098139]\n",
       " [-0.06160244 -0.0679873   0.06390848 -0.01659056 -0.03719801  0.07863112\n",
       "   0.00378838 -0.06152668  0.01966726  0.00567663]\n",
       " [-0.08636354 -0.04172058  0.04062562 -0.00541658  0.02918849  0.05975448\n",
       "  -0.068505   -0.09378979  0.05699369  0.02962759]\n",
       " [-0.08015245 -0.08177797  0.04198427 -0.00377897 -0.01149168  0.11175144\n",
       "  -0.06391674 -0.04661983  0.02155001  0.04461868]\n",
       " [-0.06704477 -0.0616311   0.0133093   0.02550123  0.0003294   0.09188411\n",
       "  -0.04664588 -0.00472781  0.04817369  0.01230503]\n",
       " [-0.10483193 -0.07711643  0.04243684 -0.02489925 -0.00598844  0.06271333\n",
       "  -0.02431563 -0.03910688  0.01781008  0.04128212]\n",
       " [-0.08944881 -0.02081486  0.06475487 -0.0271921   0.01547408  0.04438227\n",
       "  -0.05682816 -0.14115018  0.03606029  0.05409257]\n",
       " [-0.02486158 -0.06378265  0.03659953 -0.0016868   0.04077538  0.12157694\n",
       "  -0.02901412 -0.08414685  0.05180722  0.06410086]\n",
       " [-0.06805757 -0.03662756  0.03006178 -0.00958202 -0.02082584  0.08310477\n",
       "  -0.06671988 -0.06681997  0.05688377  0.02284748]\n",
       " [-0.05656666 -0.06954596  0.04473033 -0.01388742 -0.00854523  0.08002633\n",
       "  -0.00732381 -0.01918214  0.02865683  0.01605716]\n",
       " [-0.07539026 -0.08801065  0.0188965   0.01211768  0.04130932  0.05860225\n",
       "  -0.02831667 -0.06179499  0.01758333  0.05555521]\n",
       " [-0.05384695 -0.06166825 -0.02550551 -0.00483273  0.02940282  0.0847242\n",
       "  -0.02574035 -0.06368826  0.03769243  0.02729157]\n",
       " [-0.1061264  -0.05099816  0.01345661 -0.02077168 -0.04556708  0.10004036\n",
       "  -0.0133104  -0.05744222  0.04585753  0.00468992]\n",
       " [-0.05564535 -0.02021382  0.01361547 -0.02314297  0.0211111   0.02406402\n",
       "  -0.00932143 -0.05048481 -0.02409341  0.06410154]\n",
       " [-0.0561782  -0.04239519  0.04478774 -0.02917579  0.0193755   0.03311807\n",
       "  -0.02630321 -0.0782377   0.01974649  0.04050914]\n",
       " [-0.0647305  -0.05930576  0.02531737  0.01782251 -0.01059453  0.09989963\n",
       "  -0.03497312 -0.05066279  0.05888743  0.05891339]\n",
       " [-0.02866517 -0.00708102 -0.01255426 -0.02233787 -0.01128338  0.05723467\n",
       "  -0.0253703  -0.0319322   0.04954197  0.03169019]\n",
       " [-0.08483584 -0.07688463 -0.00248375 -0.00397875  0.00790231  0.08443685\n",
       "  -0.02365822 -0.076629    0.04443286  0.04223274]\n",
       " [-0.08150366 -0.01846279  0.0438996  -0.01243653 -0.03818041  0.06876072\n",
       "  -0.02484048 -0.03004139  0.03700748  0.03909789]\n",
       " [-0.05411178 -0.03626077  0.06140713 -0.0117749  -0.00833518  0.09177113\n",
       "  -0.04123095 -0.06697382 -0.00304162  0.0353475 ]\n",
       " [-0.06367737 -0.05923943  0.06569206 -0.04322957  0.043355    0.07912391\n",
       "  -0.01822791 -0.11233041 -0.0264584   0.0447478 ]\n",
       " [-0.05396599 -0.04163495  0.06418328  0.00356107  0.0088697   0.04822112\n",
       "  -0.02467759 -0.05104256  0.00839839  0.03012754]\n",
       " [-0.03235278 -0.0311615   0.01125571 -0.02588007 -0.02920821  0.06282548\n",
       "  -0.01977356 -0.02605614  0.04642238  0.02334164]\n",
       " [-0.03638164 -0.07897398  0.00696153 -0.0219533   0.06544306  0.03395649\n",
       "  -0.01007299 -0.02434244 -0.00390581  0.04679583]]\n",
       "<NDArray 64x10 @cpu(0)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.0850049313366, Train_acc 0.980466666667, Test_acc 0.983\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + smoothing_constant * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_data, net)\n",
    "    train_accuracy = evaluate_accuracy(train_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Squeeze-and-Excitation ResNets\n",
    "References:\n",
    "    - [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    "    - []() # added when paper is published on Arxiv\n",
    "'''\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import add\n",
    "from keras.layers import multiply\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras import backend as K\n",
    "\n",
    "from se import squeeze_excite_block\n",
    "\n",
    "__all__ = ['SEResNet', 'SEResNet50', 'SEResNet101', 'SEResNet154', 'preprocess_input', 'decode_predictions']\n",
    "\n",
    "\n",
    "WEIGHTS_PATH = \"\"\n",
    "WEIGHTS_PATH_NO_TOP = \"\"\n",
    "\n",
    "\n",
    "def SEResNet(input_shape=None,\n",
    "             initial_conv_filters=64,\n",
    "             depth=[3, 4, 6, 3],\n",
    "             filters=[64, 128, 256, 512],\n",
    "             width=1,\n",
    "             bottleneck=False,\n",
    "             weight_decay=1e-4,\n",
    "             include_top=True,\n",
    "             weights=None,\n",
    "             input_tensor=None,\n",
    "             pooling=None,\n",
    "             classes=1000):\n",
    "    \"\"\" Instantiate the Squeeze and Excite ResNet architecture. Note that ,\n",
    "        when using TensorFlow for best performance you should set\n",
    "        `image_data_format=\"channels_last\"` in your Keras config\n",
    "        at ~/.keras/keras.json.\n",
    "        The model are compatible with both\n",
    "        TensorFlow and Theano. The dimension ordering\n",
    "        convention used by the model is the one\n",
    "        specified in your Keras config file.\n",
    "        # Arguments\n",
    "            initial_conv_filters: number of features for the initial convolution\n",
    "            depth: number or layers in the each block, defined as a list.\n",
    "                ResNet-50  = [3, 4, 6, 3]\n",
    "                ResNet-101 = [3, 6, 23, 3]\n",
    "                ResNet-152 = [3, 8, 36, 3]\n",
    "            filter: number of filters per block, defined as a list.\n",
    "                filters = [64, 128, 256, 512\n",
    "            width: width multiplier for the network (for Wide ResNets)\n",
    "            bottleneck: adds a bottleneck conv to reduce computation\n",
    "            weight_decay: weight decay (l2 norm)\n",
    "            include_top: whether to include the fully-connected\n",
    "                layer at the top of the network.\n",
    "            weights: `None` (random initialization) or `imagenet` (trained\n",
    "                on ImageNet)\n",
    "            input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n",
    "                to use as image input for the model.\n",
    "            input_shape: optional shape tuple, only to be specified\n",
    "                if `include_top` is False (otherwise the input shape\n",
    "                has to be `(224, 224, 3)` (with `tf` dim ordering)\n",
    "                or `(3, 224, 224)` (with `th` dim ordering).\n",
    "                It should have exactly 3 inputs channels,\n",
    "                and width and height should be no smaller than 8.\n",
    "                E.g. `(200, 200, 3)` would be one valid value.\n",
    "            pooling: Optional pooling mode for feature extraction\n",
    "                when `include_top` is `False`.\n",
    "                - `None` means that the output of the model will be\n",
    "                    the 4D tensor output of the\n",
    "                    last convolutional layer.\n",
    "                - `avg` means that global average pooling\n",
    "                    will be applied to the output of the\n",
    "                    last convolutional layer, and thus\n",
    "                    the output of the model will be a 2D tensor.\n",
    "                - `max` means that global max pooling will\n",
    "                    be applied.\n",
    "            classes: optional number of classes to classify images\n",
    "                into, only to be specified if `include_top` is True, and\n",
    "                if no `weights` argument is specified.\n",
    "        # Returns\n",
    "            A Keras model instance.\n",
    "        \"\"\"\n",
    "\n",
    "    if weights not in {'imagenet', None}:\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization) or `imagenet` '\n",
    "                         '(pre-training on ImageNet).')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as imagenet with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    assert len(depth) == len(filters), \"The length of filter increment list must match the length \" \\\n",
    "                                       \"of the depth list.\"\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=32,\n",
    "                                      data_format=K.image_data_format(),\n",
    "                                      require_flatten=False)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape)\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    x = _create_se_resnet(classes, img_input, include_top, initial_conv_filters,\n",
    "                          filters, depth, width, bottleneck, weight_decay, pooling)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "    # Create model.\n",
    "    model = Model(inputs, x, name='resnext')\n",
    "\n",
    "    # load weights\n",
    "\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _resnet_block(input, filters, k=1, strides=(1, 1)):\n",
    "    ''' Adds a pre-activation resnet block without bottleneck layers\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "        k: width factor\n",
    "        strides: strides of the convolution layer\n",
    "    Returns: a keras tensor\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(input)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if strides != (1, 1) or init._keras_shape[channel_axis] != filters * k:\n",
    "        init = Conv2D(filters * k, (1, 1), padding='same', kernel_initializer='he_normal',\n",
    "                      use_bias=False, strides=strides)(x)\n",
    "\n",
    "    x = Conv2D(filters * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "               use_bias=False, strides=strides)(x)\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(filters * k, (3, 3), padding='same', kernel_initializer='he_normal',\n",
    "               use_bias=False)(x)\n",
    "\n",
    "    # squeeze and excite block\n",
    "    x = squeeze_excite_block(x)\n",
    "\n",
    "    m = add([x, init])\n",
    "    return m\n",
    "\n",
    "\n",
    "def _create_se_resnet(classes, img_input, include_top, initial_conv_filters, filters,\n",
    "                      depth, width, bottleneck, weight_decay, pooling):\n",
    "    '''Creates a SE ResNet model with specified parameters\n",
    "    Args:\n",
    "        initial_conv_filters: number of features for the initial convolution\n",
    "        include_top: Flag to include the last dense layer\n",
    "        filters: number of filters per block, defined as a list.\n",
    "            filters = [64, 128, 256, 512\n",
    "        depth: number or layers in the each block, defined as a list.\n",
    "            ResNet-50  = [3, 4, 6, 3]\n",
    "            ResNet-101 = [3, 6, 23, 3]\n",
    "            ResNet-152 = [3, 8, 36, 3]\n",
    "        width: width multiplier for network (for Wide ResNet)\n",
    "        bottleneck: adds a bottleneck conv to reduce computation\n",
    "        weight_decay: weight_decay (l2 norm)\n",
    "        pooling: Optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "    Returns: a Keras Model\n",
    "    '''\n",
    "    channel_axis = 1 if K.image_data_format() == 'channels_first' else -1\n",
    "    N = list(depth)\n",
    "\n",
    "    # block 1 (initial conv block)\n",
    "    x = Conv2D(initial_conv_filters, (7, 7), padding='same', use_bias=False, strides=(2, 2),\n",
    "               kernel_initializer='he_normal', kernel_regularizer=l2(weight_decay))(img_input)\n",
    "\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "\n",
    "    # block 2 (projection block)\n",
    "    for i in range(N[0]):\n",
    "        if bottleneck:\n",
    "            x = _resnet_bottleneck_block(x, filters[0], width)\n",
    "        else:\n",
    "            x = _resnet_block(x, filters[0], width)\n",
    "\n",
    "    # block 3 - N\n",
    "    for k in range(1, len(N)):\n",
    "        if bottleneck:\n",
    "            x = _resnet_bottleneck_block(x, filters[k], width, strides=(2, 2))\n",
    "        else:\n",
    "            x = _resnet_block(x, filters[k], width, strides=(2, 2))\n",
    "\n",
    "        for i in range(N[k] - 1):\n",
    "            if bottleneck:\n",
    "                x = _resnet_bottleneck_block(x, filters[k], width)\n",
    "            else:\n",
    "                x = _resnet_block(x, filters[k], width)\n",
    "\n",
    "    x = BatchNormalization(axis=channel_axis)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(classes, use_bias=False, kernel_regularizer=l2(weight_decay),\n",
    "                  activation='softmax')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = GlobalAveragePooling2D()(x)\n",
    "        elif pooling == 'max':\n",
    "            x = GlobalMaxPooling2D()(x)\n",
    "\n",
    "return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
